{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Represents a single layer of a neural network.\n",
    "    Layer is a 2D numpy array of the given dimensions with xavier-normalized\n",
    "    random weights and a sigmoid activation function.\n",
    "\n",
    "    - Alter someday to support non-sigmoidal nodes...\n",
    "\n",
    "    Example array:\n",
    "            Input1, Input2, Input3\n",
    "    Node1     0.3    0.15    2.7\n",
    "    Node2     0.8     1.6    5.6\n",
    "    Node3     4.1     0.9    0.1\n",
    "    Node4     3.2     2.4    0.03\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_nodes, n_inputs,\n",
    "                    layer_bias=None,\n",
    "                    activation_function=None,\n",
    "                    act_fn_name=None):\n",
    "        #the activation function for each neuron in the layer\n",
    "        sigmoid = lambda z: 1 / (1+np.exp(-z))\n",
    "        self.f = activation_function if activation_function is not None else sigmoid\n",
    "        # name for the activation function for sensible printing\n",
    "        self.f_name = act_fn_name if act_fn_name is not None else \"sigmoid\"\n",
    "        #the bias value to use for each neuron in the layer. Use default 0.1 b/c of TA conversation.\n",
    "        self.b = np.ones(n_nodes)*layer_bias if layer_bias is not None else np.ones(n_nodes)*0.1\n",
    "        #self.b = np.ones(shape=(n_nodes,1))*layer_bias if layer_bias is not None else np.ones(shape=(n_nodes,1))*0.1\n",
    "        #2D array of weights, nodes(r) x inputs(c), use xavier initialization:\n",
    "        # https://www.quora.com/What-are-good-initial-weights-in-a-neural-network\n",
    "        # and https://stackoverflow.com/questions/48641192/xavier-and-he-normal-initialization-difference\n",
    "        arr = np.random.randn(n_nodes,n_inputs) * np.sqrt(1/n_inputs)\n",
    "        self.wts = arr\n",
    "\n",
    "    def set_bias(self,new_bias,change_dimensions_okay=False):\n",
    "        \"\"\"Replaces the layer's bias term with the input value\"\"\"\n",
    "        if new_bias.shape == self.b.shape or change_dimensions_okay == True:\n",
    "            self.b = new_bias\n",
    "        else:\n",
    "            raise ValueError(\"Dimensions Mismatch with self.b disallowed: \\\n",
    "                    %s self vs %s input\" % (self.b.shape,new_bias.shape))\n",
    "\n",
    "    def set_weights_arr(self,array,change_dimensions_okay=False):\n",
    "        \"\"\"Replaces the weights array with the given array. Does not allow\n",
    "        the new array to be a different shape unless the optional arg is True\"\"\"\n",
    "        if array.shape == self.wts.shape or change_dimensions_okay == True:\n",
    "            self.wts = array\n",
    "        else:\n",
    "            raise ValueError(\"Dimensions Mismatch with self.wts disallowed: \\\n",
    "                    %s self vs %s input\" % (self.wts.shape,array.shape))\n",
    "\n",
    "\n",
    "    def feedforward_layer(self,input_values):\n",
    "        \"\"\"\n",
    "        Calculates the vector of outputs from the whole layer for a given\n",
    "        input vector:\n",
    "\n",
    "        activation_function(W0x0 + W1x1 + W2x2 + ...)  for each node in layer\n",
    "\n",
    "        Input: array-like of values, same length as the layer's number of cols\n",
    "        Output: np.array of the activation values from each neuron, given the input\n",
    "        \"\"\"\n",
    "        node_outputs = np.matmul(self.wts,input_values) + self.b\n",
    "        return self.f(node_outputs) #apply AF to z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_sanitize(input):\n",
    "    \"\"\"helper function to ensure inputs behave well for single and multiple test sets,\n",
    "    enforces [ [x,y,z] ] 2D array structure for 1D arrays so that transposition works.\"\"\"\n",
    "    return input if isinstance(input[0],(list,tuple,np.ndarray)) else np.atleast_2d([input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Represents a full neural network with sigmoidal activation functions an\n",
    "    a Euclidean squared distances loss function with regularization, starting\n",
    "    biases at 0 and xavier normalization applied to the starting random weights.\n",
    "\n",
    "    layer_sizes : array-like describing network structure, e.g. [8,3,8],\n",
    "    inputs_x : array-like for the values of the initial inputs to the network\n",
    "    true_results_y: array-like for the known answers (labels) to the given input\n",
    "    alpha: the step size\n",
    "    wd: the weight decay parameter\n",
    "    \"\"\"\n",
    "    # TODO implement sparsity ^rho in cost function??\n",
    "    def __init__(self,layer_sizes,inputs_x,true_results_y,alpha=0.5,wd=0.001,initial_bias=0.1):\n",
    "        # housekeeping for good behavior when testing multiple sets of inputs\n",
    "        self.x = array_sanitize(inputs_x) #enforces 2D structure even for only one input set\n",
    "        self.y = array_sanitize(true_results_y)\n",
    "        # check that the inputs and the outputs match the layer dimensions\n",
    "        if layer_sizes[0] != len(self.x[0]):\n",
    "            raise ValueError(\"Dimension Mismatch in Length: %s input layer vs %s input set\" %(len(layer_sizes[0]),len(self.x[0])) )\n",
    "        if layer_sizes[-1] != len(self.y[0]):\n",
    "            raise ValueError(\"Dimension Mismatch in Length: %s output layer vs %s answers set\" %(len(layer_sizes[-1]),len(self.y[0])) )\n",
    "        # set constants\n",
    "        self.alpha = alpha\n",
    "        self.wd = wd\n",
    "        self.dim = layer_sizes\n",
    "        self.init_b = initial_bias\n",
    "        # make Layers for every transition, including inputs->first layer\n",
    "        layer_dims = zip(layer_sizes[:-1],layer_sizes[1:])\n",
    "        self.layers = np.array([Layer(n,i) for i,n in layer_dims])\n",
    "\n",
    "    def __repr__(self):\n",
    "        #much easier to read printing version\n",
    "        lines = []\n",
    "        lines.append(\"x: %s\" % self.x)\n",
    "        lines.append(\"y: %s\" % self.y)\n",
    "        lines.append(\"Layers:\")\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            lines.append(\"%s -\" %i)\n",
    "            lines.append(layer.__repr__())\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "    def feedforward_single(self,input):\n",
    "        \"\"\"\n",
    "        implements feeding forward for a single set of inputs\n",
    "\n",
    "        Input: 1D array of input values, optional flag to return all activations\n",
    "        \n",
    "        Output: Tuple of (array of outputs, array of activations for each layer)\n",
    "                NOTE: the first array of activations is the inputs!        \n",
    "        \"\"\"\n",
    "        activation = np.array([np.zeros(n) for n in self.dim])\n",
    "        activation[0] = input\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            output = layer.feedforward_layer(input)\n",
    "            activation[i+1] = output\n",
    "            input = output\n",
    "        return output,activation\n",
    "\n",
    "\n",
    "    def feedforward(self,new_x=None):\n",
    "        \"\"\"\n",
    "        implements feeding forward through the whole network\n",
    "\n",
    "        Input: optional array of inputs, \n",
    "               (if no inputs given, use self.x)\n",
    "               \n",
    "        Output: Tuple of (outputs f.a. input sets, activations f.a. input sets)\n",
    "                NOTE: the first array of each activations is the inputs!\n",
    "        \"\"\"\n",
    "        #allow but don't require passing in new inputs, copy so we don't alter\n",
    "        input = array_sanitize(np.copy(new_x)) if new_x is not None else np.copy(self.x)\n",
    "        #initialize array of outputs\n",
    "        results = np.array([ np.zeros(len(self.y[0])) for m in input])\n",
    "        #make array of activation arrays, one for each layer & inputs\n",
    "        activations = np.array([np.array([np.zeros(n) for n in self.dim]) for m in input])\n",
    "        \n",
    "        for m,input_set in enumerate(input): # do this for every test\n",
    "            results[m],activations[m] = self.feedforward_single(input_set)\n",
    "        return results,activations\n",
    "\n",
    "\n",
    "    def backpropagate(self,new_y=None,new_x=None):\n",
    "        \"\"\"\n",
    "        implements backpropagating error and updating weights through the whole\n",
    "        network\n",
    "\n",
    "        Inputs: optional array of true outputs and optional array of inputs\n",
    "                (if not given, uses self.y and self.x respectively)\n",
    "        Outputs: None (updates weights and biases in self.layers)\n",
    "        \"\"\"\n",
    "        #allow but don't require passing in true output labels, copy for safety, ensure 2D\n",
    "        true_ys = array_sanitize(np.copy(new_y)) if new_y is not None else np.copy(self.y)\n",
    "        inputs = array_sanitize(np.copy(new_x)) if new_x is not None else np.copy(self.x)\n",
    "        outputs,activations = self.feedforward(inputs)\n",
    "\n",
    "        # initialize arrays to hold partial derivatives of weights and biases\n",
    "        grad_weights= np.array([ np.zeros(l.wts.shape) for l in self.layers ])\n",
    "        grad_biases = np.array([ np.zeros(l.b.shape) for l in self.layers ])\n",
    "        \n",
    "        # initialize array to hold deltas for nodes in each layer, get outer delta\n",
    "        deltas = np.array([ np.zeros(len(l.wts)) for l in self.layers ])\n",
    "\n",
    "        # accumulate partial derivatives for every input set\n",
    "        for output,activation,true_y in zip(outputs,activations,true_ys):\n",
    "            \n",
    "            # calculate delta and partial derivatives for output layer\n",
    "            delta_outer = np.multiply( (output-true_y),( output*(1-output) ))\n",
    "            grad_weights[-1] += np.array([activation[-2]])*np.array([delta_outer]).T #cast 1D as 2D to ensure proper multiplication\n",
    "            grad_biases[-1] += delta_outer\n",
    "            deltas[-1] = delta_outer\n",
    "            \n",
    "            # calcluate delta and partial derivatives for all inner layers, working backwards\n",
    "            for i in range(len(self.layers)-2,-1,-1):\n",
    "                l,next_l = self.layers[i],self.layers[i+1]\n",
    "                a,prev_a = activation[i+1],activation[i] #activation[0] is input\n",
    "                # calculate delta\n",
    "                delta = np.multiply( np.matmul(next_l.wts.T,deltas[i+1]), (a*(1-a)) )\n",
    "                deltas[i] = delta\n",
    "                # calculate partial derivatives for weights and bias in the layer\n",
    "                grad_weights[i] += np.atleast_2d([prev_a])*np.atleast_2d([delta]).T #cast both 1Ds as 2D to ensure proper matrix multiplication\n",
    "                grad_biases[i] += delta\n",
    "\n",
    "        # Iterate through all layers forward, updating weights\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            weight_decay_term = self.wd*layer.wts\n",
    "            error_term = (1/len(inputs)) * grad_weights[i]\n",
    "            updated_wts = layer.wts - self.alpha*(error_term+weight_decay_term)\n",
    "            updated_bias = layer.b - self.alpha*(1/len(inputs))*grad_biases[i]\n",
    "            layer.set_weights_arr(updated_wts)\n",
    "            layer.set_bias(updated_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = [[1,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0],[0,0,1,0,0,0,0,0],[0,0,0,1,0,0,0,0],\n",
    "        [0,0,0,0,1,0,0,0],[0,0,0,0,0,1,0,0],[0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,1] ]\n",
    "encoder_outputs = encoder_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**ITERATION 1000 **\n",
      "[0.3 0.  0.  0.  0.  0.  0.1 0.1]\n",
      "[0.1 0.6 0.  0.2 0.  0.1 0.2 0. ]\n",
      "[0.2 0.  0.8 0.1 0.2 0.  0.2 0. ]\n",
      "[0.1 0.2 0.1 0.7 0.1 0.  0.  0.1]\n",
      "[0.2 0.  0.1 0.2 0.6 0.  0.  0.3]\n",
      "[0.2 0.2 0.  0.  0.  0.7 0.  0.3]\n",
      "[0.2 0.2 0.1 0.  0.  0.  0.7 0. ]\n",
      "[0.2 0.  0.  0.  0.3 0.2 0.  0.6]\n",
      "\n",
      "\n",
      "**ITERATION 2000 **\n",
      "[0.8 0.  0.  0.  0.  0.  0.1 0. ]\n",
      "[0.  0.8 0.1 0.2 0.  0.1 0.2 0. ]\n",
      "[0.1 0.  0.9 0.  0.1 0.  0.1 0. ]\n",
      "[0.  0.2 0.1 0.8 0.1 0.  0.  0. ]\n",
      "[0.  0.  0.1 0.1 0.8 0.  0.  0.1]\n",
      "[0.1 0.1 0.  0.  0.  0.8 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.  0.  0.1 0.8 0. ]\n",
      "[0.1 0.  0.  0.  0.1 0.1 0.  0.8]\n",
      "\n",
      "\n",
      "**ITERATION 3000 **\n",
      "[0.9 0.  0.  0.  0.  0.  0.1 0. ]\n",
      "[0.  0.8 0.1 0.1 0.  0.1 0.1 0. ]\n",
      "[0.1 0.  0.9 0.  0.1 0.  0.1 0. ]\n",
      "[0.  0.1 0.  0.8 0.1 0.  0.  0. ]\n",
      "[0.  0.  0.1 0.1 0.8 0.  0.  0.1]\n",
      "[0.  0.  0.  0.  0.  0.9 0.1 0.1]\n",
      "[0.1 0.1 0.1 0.  0.  0.1 0.8 0. ]\n",
      "[0.1 0.  0.  0.  0.1 0.1 0.  0.8]\n",
      "\n",
      "\n",
      "**ITERATION 4000 **\n",
      "[0.9 0.  0.  0.  0.  0.  0.1 0. ]\n",
      "[0.  0.8 0.1 0.1 0.  0.  0.1 0. ]\n",
      "[0.1 0.  0.9 0.  0.1 0.  0.1 0. ]\n",
      "[0.  0.1 0.  0.8 0.1 0.  0.  0. ]\n",
      "[0.  0.  0.1 0.1 0.8 0.  0.  0.1]\n",
      "[0.  0.  0.  0.  0.  0.9 0.1 0.1]\n",
      "[0.1 0.1 0.  0.  0.  0.1 0.8 0. ]\n",
      "[0.1 0.  0.  0.  0.1 0.1 0.  0.9]\n",
      "\n",
      "\n",
      "**ITERATION 5000 **\n",
      "[0.9 0.  0.  0.  0.  0.  0.1 0. ]\n",
      "[0.  0.9 0.1 0.1 0.  0.  0.1 0. ]\n",
      "[0.1 0.  0.9 0.  0.1 0.  0.1 0. ]\n",
      "[0.  0.1 0.  0.8 0.1 0.  0.  0. ]\n",
      "[0.  0.  0.1 0.1 0.8 0.  0.  0.1]\n",
      "[0.  0.  0.  0.  0.  0.9 0.1 0.1]\n",
      "[0.1 0.1 0.  0.  0.  0.1 0.8 0. ]\n",
      "[0.1 0.  0.  0.  0.1 0.1 0.  0.9]\n",
      "\n",
      "\n",
      "**ITERATION 6000 **\n",
      "[0.9 0.  0.  0.  0.  0.  0.1 0. ]\n",
      "[0.  0.9 0.1 0.1 0.  0.  0.1 0. ]\n",
      "[0.1 0.  0.9 0.  0.1 0.  0.1 0. ]\n",
      "[0.  0.1 0.  0.8 0.1 0.  0.  0. ]\n",
      "[0.  0.  0.1 0.1 0.8 0.  0.  0.1]\n",
      "[0.  0.  0.  0.  0.  0.9 0.1 0.1]\n",
      "[0.1 0.1 0.  0.  0.  0.1 0.9 0. ]\n",
      "[0.1 0.  0.  0.  0.1 0.1 0.  0.9]\n",
      "\n",
      "\n",
      "**ITERATION 7000 **\n",
      "[0.9 0.  0.  0.  0.  0.  0.1 0.1]\n",
      "[0.  0.9 0.1 0.1 0.  0.  0.1 0. ]\n",
      "[0.1 0.1 0.9 0.  0.1 0.  0.  0. ]\n",
      "[0.  0.1 0.  0.8 0.1 0.  0.  0. ]\n",
      "[0.  0.  0.1 0.1 0.9 0.  0.  0.1]\n",
      "[0.  0.  0.  0.  0.  0.9 0.1 0.1]\n",
      "[0.1 0.1 0.  0.  0.  0.1 0.9 0. ]\n",
      "[0.1 0.  0.  0.  0.1 0.1 0.  0.9]\n",
      "\n",
      "\n",
      "**ITERATION 8000 **\n",
      "[0.9 0.  0.1 0.  0.  0.  0.1 0.1]\n",
      "[0.  0.9 0.1 0.1 0.  0.  0.1 0. ]\n",
      "[0.1 0.1 0.9 0.  0.1 0.  0.  0. ]\n",
      "[0.  0.1 0.  0.9 0.1 0.  0.  0. ]\n",
      "[0.  0.  0.1 0.1 0.9 0.  0.  0.1]\n",
      "[0.  0.  0.  0.1 0.  0.9 0.1 0.1]\n",
      "[0.1 0.1 0.  0.  0.  0.1 0.9 0. ]\n",
      "[0.1 0.  0.  0.  0.1 0.1 0.  0.9]\n",
      "\n",
      "\n",
      "**ITERATION 9000 **\n",
      "[0.9 0.  0.1 0.  0.  0.  0.1 0.1]\n",
      "[0.  0.9 0.1 0.1 0.  0.  0.1 0. ]\n",
      "[0.1 0.1 0.9 0.  0.1 0.  0.  0. ]\n",
      "[0.  0.1 0.  0.9 0.1 0.  0.  0. ]\n",
      "[0.  0.  0.1 0.1 0.9 0.  0.  0.1]\n",
      "[0.  0.  0.  0.1 0.  0.9 0.1 0.1]\n",
      "[0.1 0.1 0.  0.  0.  0.1 0.9 0. ]\n",
      "[0.1 0.  0.  0.  0.1 0.1 0.  0.9]\n",
      "\n",
      "\n",
      "**ITERATION 10000 **\n",
      "[0.9 0.  0.1 0.  0.  0.  0.1 0.1]\n",
      "[0.  0.9 0.1 0.1 0.  0.  0.1 0. ]\n",
      "[0.1 0.1 0.9 0.  0.1 0.  0.  0. ]\n",
      "[0.  0.1 0.  0.9 0.1 0.  0.  0. ]\n",
      "[0.  0.  0.1 0.1 0.9 0.  0.  0.1]\n",
      "[0.  0.  0.  0.1 0.  0.9 0.1 0.1]\n",
      "[0.1 0.1 0.  0.  0.  0.1 0.9 0. ]\n",
      "[0.1 0.  0.  0.  0.1 0.1 0.  0.9]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_net = NeuralNetwork([8,3,8],encoder_inputs,encoder_outputs,alpha=2,wd=0.0001)\n",
    "for i in range(1,10001):\n",
    "    test_net.backpropagate()\n",
    "    if i%1000 == 0:\n",
    "        print('**ITERATION',i,\"**\")\n",
    "        results,activations = test_net.feedforward(new_x=encoder_inputs)\n",
    "        for r in results:\n",
    "            print(r.round(1))            \n",
    "        print()\n",
    "#         for i,l in enumerate(test_net.layers):\n",
    "#             print('layer',i)\n",
    "#             print(l.wts.round(3))\n",
    "#             print(l.b)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
